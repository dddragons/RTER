{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Highly Relevant Textual Evidence Recognition\n",
    "---\n",
    "In this study, we develop a novel approach to automatically identifying highly relevant textual evidence (or text mentions) for relations in a structured knowledge base. \n",
    "\n",
    ">Distant supervision for relation extraction (DSRE) first gathers training examples by aligning the relational instances in an existing structured knowledge base with free text. The distant supervision approach then uses these noisy examples to build machine learning models for extracting additional structured information from free text. Distant supervision for relation extraction (DSRE) approach has attracted an increasing attention in NLP and knowledge engineering related fields. Various methods and techniques have been developed for dealing with the problem of noisy training examples that are automatically identified. \n",
    "\n",
    "---\n",
    "\n",
    "**References:**\n",
    "\n",
    "[Mintz 2009] Mike Mintz, Steven Bills, Rion Snow, and Daniel Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics.\n",
    "\n",
    "[Riedel 2010] Sebastian Riedel, Limin Yao, and Andrew McCallum. 2010. Modeling relations and their mentions without labeled text. In Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases (ECML PKDD ’10).\n",
    "\n",
    "[Hoffman 2011] Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, and Daniel S. Weld. 2011. Knowledgebased weak supervision for information extraction of overlapping relations. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).\n",
    "\n",
    "[Nguyen 2011] Truc Vien T. Nguyen and Alessandro Moschitti. 2011. End-to-end relation extraction using distant supervision from external semantic repositories. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies.\n",
    "\n",
    "[Sun 2011] Ang Sun, Ralph Grishman, Wei Xu, and Bonan Min. 2011. New York University 2011 system for KBP slot filling. In Proceedings of the Text Analytics Conference.\n",
    "\n",
    "[Surdeanu 2011] Mihai Surdeanu, Sonal Gupta, John Bauer, David Mc- Closky, Angel X. Chang, Valentin I. Spitkovsky, and Christopher D. Manning. 2011a. Stanford’s distantlysupervised slot-filling system. In Proceedings of the Text Analytics Conference.\n",
    "\n",
    "[Surdeanu 2012] Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati, and Christopher D Manning. 2012. Multi-instance multi-label learning for relation extraction. In Proceedings of EMNLP, pages 455–465.\n",
    "\n",
    "[Zeng et al., 2014] Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou, and Jun Zhao. Relation classification via convolutional deep neural network. In Proceedings of COLING.\n",
    "\n",
    "[Zeng et al.,2015] Daojian Zeng,Kang Liu,Yubo Chen,and Jun Zhao. Distant supervision for relation extraction via piecewise convolutional neural networks. In Proceedings of EMNLP.\n",
    "\n",
    "[Zhou et al.,2016] Zhou P, Shi W, Tian J, et al. Attention-Based Bidirectional Long Short-Term Memory Networks for Relation Classification. Meeting of the Association for Computational Linguistics. 2016:207-212."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reviews on Distant Supervision for Relation Extraction:\n",
    "_[Nickel 2015] M. Nickel, K. Murphy, V. Tresp, and E. Gabrilovich. A review of relational machine learning for knowledge graphs. In eprint arXiv:1503.00759, 03/2015._\n",
    "\n",
    ">**Abstract**: Relational machine learning studies methods for the statistical analysis of relational, or graph-structured, data. In this paper, we provide a review of how such statistical models can be \"trained\" on large knowledge graphs, and then used to predict new facts about the world (which is equivalent to predicting new edges in the graph). In particular, we discuss two fundamentally different kinds of statistical relational models, both of which can scale to massive datasets. The first is based on latent feature models such as tensor factorization and multiway neural networks. The second is based on mining observable patterns in the graph. We also show how to combine these latent and observable models to get improved modeling power at decreased computational cost. Finally, we discuss how such statistical models of graphs can be combined with text-based information extraction methods for automatically constructing knowledge graphs from the Web. To this end, we also discuss Google's Knowledge Vault project as an example of such combination.\n",
    "\n",
    "_[Kumar 2017] Shantanu Kumar. A Survey of Deep Learning Methods for Relation Extraction. eprint arXiv:1705.03645_\n",
    "\n",
    ">**Abstract**: Relation Extraction is an important sub-task of Information Extraction which has the potential of employing deep learning (DL) models with the creation of large datasets using distant supervision. In this review, we compare the contributions and pitfalls of the various DL models that have been used for the task, to help guide the path ahead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem\n",
    "**_How to identify highly relevant textual evidence for the relational instances from the noisy set of sentences obtained by an initial alignment?_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Related Work**\n",
    "\n",
    "[Mintz 2009] proposes the distant supervision approach for relation extraction. The method aligns plain text with Freebase instances for gathering training examples. They assume any sentences mentioning the entities in the relation instances express the relations. To alleviate the wrong labelling problem, [Riedel 2010] models distant supervision for relation extraction as a multi-instance single-label problem. Furthermore,  [Hoffmann 2011; Surdeanu 2012] adopt multi-instance and multi-label learning for distant supervision for relation extraction.\n",
    "\n",
    "Multi-instance learning was originally proposed to address the issue of ambiguously-labelled training data when predicting the activity of drugs [Dietterich\n",
    "et al., 1997]. Multi-instance learning considers the reliability of the labels for each instance. [Bunescu and Mooney, 2007] connects weak supervision with multi-instance learning and extends it to relation extraction. \n",
    "\n",
    "Methods of using neural networks have proposed to automatically learn features for relation extraction. [Xie 2016] attempts to incorporate the text information of entities\n",
    "for relation extraction. [Zeng 2015] combines at-least-one multi-instance learning\n",
    "with neural network model to extract relations on distant supervision data. However, they assume that only one sentence is active for each entity pair.\n",
    "Hence, it will lose a large amount of rich information containing in those neglected sentences. [Lin et al. 2016] propose sentencelevel attention over multiple instances, which can utilize all informative sentences. The method is expected to dynamically\n",
    "reduce the weights of those noisy instances. The method extracts relation with the relation vector weighted by sentence-level attention.\n",
    " \n",
    "When people deal with the noisy data for training a extractor, they treat the problem using traditional multi-instance multi-label (MIML) learning. MIML doesn't attempt to clean up the noisy training examples, but tries to learn a model from the noisy data. We aim to clean up the noisy data based on latent features in the KBs. We believe a set of cleaned-up training examples will allow to train models with better performance.\n",
    "\n",
    "[Surdeanu 2012] proposes a graphical model for solving the MIML problem as shown in the  following figure:\n",
    "<img src=\"images/Surdeanu-MIML-model.png\" style=\"width:300px;\">\n",
    "\n",
    ">where,<br/> \n",
    "$n$ is the number of relation instances in the structured KB; <br/>\n",
    "$M_i$ is the number of mentions for the $i$th instance; <br/>\n",
    "$x$ is a sentence and $z$ is the latent relation classification for the sentence; <br />\n",
    "$\\mathbf{w_z}$ is the vector for the mention-level multi-label classifier; <br/>\n",
    "$k$ is the number of relation labels; <br/>\n",
    "$y_j$ is the top level classification for the relation instance; <br/>\n",
    "$\\mathbf{w_j}$ is the vector for the top-level binary classifier. <br/>\n",
    "\n",
    ">The log likelihood of the data is:\n",
    "\\begin{equation}\n",
    "LL(\\mathbf{w_z, w_y}) = \\sum_{i=1}^{n} log p(y_i \\mid \\mathbf{x_i}, \\mathbf{w_z, w_y}) = \n",
    "\\sum_{i=1}^{n} log \\sum_{\\mathbf{z_i}} p(y_i, \\mathbf{z_i} \\mid \\mathbf{x_i}, \\mathbf{w_z, w_y})\n",
    "\\end{equation}\n",
    "\n",
    ">By an EM algorithm, the inference becomes: <br/>\n",
    "\\begin{equation}\n",
    "z_i^{(m)} = \\arg \\max_{z} p(z \\mid x^{(m)}, \\mathbf{w_z})\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "y_i^{(r)} = \\arg \\max_{y = \\{0, 1\\}} p(y \\mid \\mathbf{z_i, w_y^{(r)}})\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "[Angeli 2014] provides partial supervision to distantly supervised relation extraction using a small set of carefully selected examples.\n",
    "\n",
    "\n",
    "[Han 2016] proposes to use supplemental knowledge for the distant supervision and joint inference across relation instances. They use _Markov Logic_ as their modeling and representation language. \n",
    ">One common solution of label uncertainty is to use multiinstance learning techniques, which can exploit the dependencies between the labels of an entity pair and the labels of\n",
    "its mentions (Bunescu and Mooney, 2007; Riedel et al., 2010; Yao et al., 2010). Hoffmann et al. (2010) and Surdeanu et al. (2012) extended the multi-instance model into multi-instance multi-label model so that an entity pair can have multiple labels. Xu et al. (2013), Min et al. (2013), Ritter et al. (2013) and Zhang et al. (2013) further extended the model to resolve the incompleteness of KB. The main drawback of these methods is that they only exploit one specific kind of indirect supervision knowledge, but ignore many other kinds of useful supervision knowledge. \n",
    "\n",
    ">There were also some DS methods which try to eliminate wrongly labeled instances using additional knowledge. Takamatsu et al. (2012), Roth and Klakow (2013) and Han and Sun (2014) exploited the distributional statistics of instances/\n",
    "patterns/features. Hoffmann et al. (2010) and Zhang et al. (2010) focused on learning dynamic lexicon. Nguyen and Moschitti (2011) and Pershina et al. (2014) constructed\n",
    "extractors by infusing labeled corpus with heuristically labeled corpus of DS. Riedel et al. (2013) and Fan et al. (2014) exploited the co-occurrence statistics between relation types/instances/features. The main drawback of these methods is that they are specialized to the used supervision knowledge, cannot exploit different kinds of indirect supervision\n",
    "knowledge together.\n",
    "\n",
    "[Han 2016] first builds a base MLN which encodes the extraction rules using the alignment between the relation instances and text. the key to the distant supervision is:\n",
    "> HasRel($p,+r$) $\\wedge$ AtLeastOne($M_p,+r$)\n",
    "\n",
    "The above base MLN is augumented by (1) dependencies between relation labels, for example, $\\texttt{Capital_of => City_of}$, (2) consistency between a relation type and its arguments, (3) nearst neighbor consistency assumption. Their experiments showed that label dependencies have the most impact on the performance.\n",
    "\n",
    "---\n",
    "**References**:\n",
    "\n",
    "[Dietterich 1997] Thomas G Dietterich, Richard H Lathrop, and Tom´as Lozano-P´erez. 1997. Solving the multiple instance problem with axis-parallel rectangles. Artificial intelligence, 89(1):31–71.\n",
    "\n",
    "[Bunescu and Mooney 2007] Razvan Bunescu and Raymond Mooney. 2007. Learning to extract relations from the web using minimal supervision. In Proceedings of ACL, volume 45, page 576.\n",
    "\n",
    "[Xie 2016] Ruobing Xie, Zhiyuan Liu, Jia Jia, Huanbo Luan, and Maosong Sun. 2016. Representation learning of knowledge graphs with entity descriptions.\n",
    "\n",
    "[Angeli 2014] G. Angeli, J. Tibshirani, J. Y. Wu, and C. D. Manning. Combining distant and partial supervision for relation extraction. In Empirical Methods in Natural Language Processing (EMNLP), October 2014.\n",
    "\n",
    "[Han 2016] X. Han and L. Sun. Global distant supervision for relation extraction. In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, AAAI’16, pages 2950–2956. AAAI Press, 2016."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Related Work (con't)**:\n",
    "\n",
    "[Han 2014] proposes a method considering the semantic consistency for DS. It can effectively identify reliable instances from noisy instances by inspecting whether an instance is located in a semantically consistent region. It models a textual mention (instance) as a linear combination of all the other instances identified through DS alignment. It then chooses the instance with highly consistent components as a reliable instance. The key is the solution to the following linear equation:\n",
    ">$y = Ax$, where $A$ is the concatenated matrix of the feature vetors of all the training instances. \n",
    "\n",
    "For practical issue, the authors only consider K nearst neighbors (K=1000).\n",
    "\n",
    "[Ye 2016] uses the notion of class ties in DSRE. Class ties mean the connections between relations in relation extraction. In general, we conclude that class ties can have two types: weak class ties and strong class ties. Joint extraction exploits class ties between relations and pairwise ranking classify positive labels from negative ones. They first use CNN to embed sentences, then introduce two variant methods to combine the embedded sentences into one bag representation vector aiming to aggregate information across sentences, after that they measure the similarity between bag representation and relation class in realvalued space.\n",
    "<img src=\"images/joint-extraction-class-tie.png\" style=\"width:300px;\">\n",
    " \n",
    "The information across all sentences is combined by ATT or AVE as introduced in [Lin 2016]\n",
    "\n",
    "[Santos 2015] proposes a class-ranking CNN for relation classification. \n",
    "<img src=\"images/class-ranking-CNN.png\" style=\"width:300px;\">\n",
    "\n",
    "The CR-CNN learn word embeddings, position embeddings, and class embeddings jointly as it classifies relations. \n",
    "\n",
    "[Zeng et al.,2015] proposes a Piecewise Convolutional Neural Networks (PCNNs) with multi-instance learning. Distant supervised relation extraction is treated as a multi-instance problem, where the training set consists of many bags, and each contains many instances. The labels of the bags are known; however, the labels of the instances\n",
    "in the bags are unknown. We design an objective function at the bag level. In the learning process, the uncertainty of instance labels can be taken into account; this alleviates the wrong label problem. To address the second problem, we adopt convolutional architecture to automatically learn relevant features without complicated NLP preprocessing.\n",
    "<img src=\"images/PCNN.png\" style=\"width:500px;\">\n",
    "\n",
    "For the multi-instance training, the final loss function is defined on the bag rather than the individual instances inside the bag. Given all ($T$) training bags $(M_i, y_i)$, we can define the objective function using cross-entropy at the bag level as follows:\n",
    ">$J (\\theta) = \\sum_1^T log p(y_i \\mid m_i^j; \\theta)$ \n",
    "\n",
    "where $j$ is constrained as follows:\n",
    "\n",
    ">$j^* = \\arg \\max_j p(y_i\\mid m_i^j ; \\theta), 1\\leq j \\leq q_i$\n",
    "\n",
    "[Fan 2014] formulates the relation extraction task from a novel perspective of using\n",
    "matrix completion with low rank criterion. They model the task with a sparse matrix whose rows present items (entity pairs) and columns contain noisy textual features and incomplete relation labels. In such a way, relation classification is transformed\n",
    "into a problem of completing the unknown labels for testing items in the sparse matrix that concatenates training and testing textual features with training labels, based on the assumption that the item-by-feature and item-by-label joint matrix is of low rank. The rationale of this assumption is that noisy features and incomplete labels are\n",
    "semantically correlated. The low-rank factorization of the sparse feature-label matrix delivers the low-dimensional representation of de-correlation for features and labels.\n",
    "\n",
    "**Drawbacks:** First, they can not process new coming testing items efficiently, as we have to reconstruct the data matrix containing not only the testing items but also all the training items for relation classification, and compute in iterative fashion again. Second, the volume of the datasets we adopt are relatively small. For the future work, we plan to improve our models so that they will be capable of incremental learning on large-scale datasets (Chang, 2011).\n",
    "\n",
    "[Riedel 2013] presents relation extraction into universal schemas. Such schemas contain surface patterns as relations, as well as relations from structured sources. By predicting missing tuples for surface pattern relations we can populate a database without any labelled data, and answer questions not sup- ported by the structured schema alone. By predicting missing tuples in the structured schema we can expand a knowledge base of fixed schema, and only require a set of existing facts from this schema. Crucially, by predicting and modeling both surface patterns and structured relations simultaneously we can improve performance. All data annotations are available at http://www.riedelcastro.org/uschema\n",
    "\n",
    "[Lin 2016] proposes a sentence-level attention-based model for relation extraction.\n",
    "In this model, we employ convolutional neural networks to embed the semantics\n",
    "of sentences. Afterwards, they build sentence-level attention over multiple instances, which is expected to dynamically reduce the weights of those noisy instances.\n",
    "<img src=\"images/CNN-sentence-attention.png\" style=\"width:300px;\">\n",
    "\n",
    "\n",
    "**References:**\n",
    "\n",
    "[Han 2014] X. Han and L. Sun. Semantic consistency: A local subspace based method for distant supervised relation extraction. In ACL, pages 718–724, 2014.\n",
    "\n",
    "[Ye 2016] H. Ye, W. Chao, and Z. Luo. Jointly extracting relations with class ties via effective deep ranking. CoRR, abs/1612.07602, 2016.\n",
    "\n",
    "[Santos 2015] C. N. dos Santos, B. Xiang, and B. Zhou. Classifying relations by ranking with convolutional neural networks. CoRR, abs/1504.06580, 2015.\n",
    "\n",
    "[Zeng et al.,2015] Daojian Zeng,Kang Liu,Yubo Chen,and Jun Zhao. Distant supervision for relation extraction via piecewise convolutional neural networks. In Proceedings of EMNLP.\n",
    "\n",
    "[Fan 2014] Fan, M., et al. 2014. Distant Supervision for Relation Extraction with Matrix Completion. In: Proceedings of ACL 2014, pp. 839- 849.\n",
    "\n",
    "[Riedel 2013] Riedel, S., Yao, L., et al. 2013. Relation Extraction with Matrix Factorization and Universal Schemas. In: Proceedings of NAACL-HLT 2013, pp. 74-84.\n",
    "\n",
    "[Lin et al., 2016] Yankai Lin, Shiqi Shen, Zhiyuan Liu, Huanbo Luan, and Maosong Sun. Neural Relation Extraction with Selective Attention over Instances. In Proceedings of ACL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Related Work (con't)**:\n",
    "\n",
    ">For the assumption that the knowledge base is complete: entity mentions with no known relation are treated as negative examples, [Min et al. (2013)] address the assumption by extending the MIML model with additional latent variables, while [Xu et al. (2013)] allow feedback from a coarse relation extractor to augment labels from the knowledge base. \n",
    "\n",
    "[Min 2013] B. Min, R. Grishman, L. Wan, C. Wang, and D. Gondek. Distant supervision for relation extraction with an incomplete knowledge base. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 777–782, Atlanta, Georgia, June 2013. Association for Computational Linguistics.\n",
    "\n",
    "[Xu 2013] W. Xu, R. Hoffmann, L. Zhao, and R. Grishman. Filling knowledge base gaps for distant supervision of relation extraction, volume 2, pages 665–670. Association for Computational Linguistics (ACL), 2013."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence and Document Embeddings\n",
    " \n",
    "[Xie 2015] J. Xie, R. B. Girshick, and A. Farhadi. Unsupervised deep embedding for clustering analysis. CoRR, abs/1511.06335, 2015.\n",
    "\n",
    "<img src=\"images/multilayer-deep-autoencoder.png\" style=\"width:500px;\">\n",
    "\n",
    "\n",
    "[Le 2013] Le, Quoc V. Building high-level features using large scale unsupervised learning. In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pp. 8595–8598. IEEE, 2013.\n",
    "\n",
    "<img src=\"images/sparse-deep-autoencoder.png\" style=\"width:500px;\">\n",
    "\n",
    "\n",
    "[Wiki2vec] Generating Vectors for DBpedia Entities via Word2Vec and Wikipedia Dumps, https://github.com/idio/wiki2vec\n",
    "\n",
    "[Gensim] models.doc2vec – Deep learning with paragraph2vec https://radimrehurek.com/gensim/models/doc2vec.html\n",
    "\n",
    "[Zhang 2015] X. Zhang and Y. LeCun. Text understanding from scratch. CoRR, abs/1502.01710, 2015.\n",
    ">**This is a text classification study!** In this article we show that text understanding can be handled\n",
    "by a deep learning system without artificially embedding\n",
    "knowledge about words, phrases, sentences or any\n",
    "other syntactic or semantic structures associated with a language.\n",
    "We apply temporal ConvNets(LeCun et al., 1998)\n",
    "to various large-scale text understanding tasks, in which the\n",
    "inputs are quantized characters and the outputs are abstract\n",
    "properties of the text. **This work has done synonym replacements as well.** It uses the  [LibreOffice1 project](http://www.libreoffice.org/).\n",
    "\n",
    "<img src=\"images/ConvNets.png\" style=\"width:500px;\">\n",
    "\n",
    "[Le 2014] Q. V. Le and T. Mikolov. Distributed representations of sentences and documents. CoRR, abs/1405.4053, 2014.\n",
    "\n",
    "<img src=\"images/paragraph-vectors.png\" style=\"width:500px;\">\n",
    "\n",
    "\n",
    "[Lau 2016] J. H. Lau and T. Baldwin. An empirical evaluation of doc2vec with practical insights into document embedding generation. CoRR, abs/1607.05368, 2016.\n",
    ">Recently, Le and Mikolov (2014) proposed doc2vec as an extension to word2vec (Mikolov et al., 2013a) to learn document-level embeddings. Despite promising results in the original paper, others have struggled to reproduce those results. This paper presents a rigorous\n",
    "empirical evaluation of doc2vec over two tasks. We compare doc2vec\n",
    "to two baselines and two state-of-the-art document embedding methodologies. We\n",
    "found that doc2vec performs robustly when using models trained on large external\n",
    "corpora, and can be further improved by using pre-trained word embeddings.\n",
    "We also provide recommendations on hyper-parameter settings for generalpurpose\n",
    "applications, and release source code to induce document embeddings using our trained doc2vec models.\n",
    "\n",
    "\n",
    "[Dai 2015] A. M. Dai, C. Olah, and Q. V. Le. Document embedding with paragraph vectors. CoRR, abs/1507.07998, 2015.\n",
    "\n",
    ">Here we consider tasks other than sentiment analysis, provide a more thorough comparison of Paragraph Vectors to other document modelling algorithms such as Latent Dirichlet Allocation, and evaluate performance of the method as we vary the dimensionality of the learned representation. We benchmarked the models on two document similarity data sets, one from Wikipedia, one from arXiv. We observe that the Paragraph Vector method performs significantly better than other methods, and propose a simple improvement to enhance embedding quality. Somewhat surprisingly, we also show that much like word embeddings, vector operations on Paragraph Vectors can perform useful semantic results.\n",
    "\n",
    "[Palangi 2015] H. Palangi, L. Deng, Y. Shen, J. Gao, X. He, J. Chen, X. Song, and R. K. Ward. Deep sentence embedding using the long short term memory network: Analysis and application to information retrieval. CoRR, abs/1502.06922, 2015.\n",
    "<img src=\"images/RNN-LSTM-sentence-embeddings.png\" style=\"width:600px;\">\n",
    "\n",
    "<img src=\"images/RNN-LSTM-sentence-embeddings-architecture.png\" style=\"width:600px;\">\n",
    "\n",
    "[Kiros 2015] R. Kiros, Y. Zhu, R. Salakhutdinov, R. S. Zemel, A. Torralba, R. Urtasun, and S. Fidler. Skip-thought vectors. CoRR, abs/1506.06726, 2015.\n",
    "\n",
    "**Encoder.** Let $w_i^1 \\dots w_i^N$ be the words in sentence $s_i$ where $N$ is the number of words in the sentence. At each time step, the encoder produces a hidden state $h_i^t\n",
    "$ which can be interpreted as the representation of the sequence $w_i^1 \\dots w_i^t$. The hidden state $h_i^N$ thus represents the full sentence. To encode a sentence, we iterate the following sequence of equations (dropping the subscript $i$):\n",
    "\n",
    ">>(1) $r^t = \\sigma(W_r x^t + U_r h^{t-1})$ \n",
    "\n",
    ">>(2) $z^t = \\sigma(W_z x^t + U_z h^{t-1})$ \n",
    "\n",
    ">>(3) $\\overline{h^t} = tanh(W x^t + U(r^t \\odot h^{t-1}))$\n",
    "\n",
    ">>(4) $h^t = (1 - z^t) \\odot h^{t-1} + z^t \\odot \\overline{h^t}$ \n",
    "\n",
    "\n",
    "> where $\\overline{h^t}$ is the proposed state update at time $t$, $z^t$ is the update gate, $r^t$ is the reset gate, ($\\odot$) denotes a component-wise product. Both update gates takes values between zero and one.\n",
    "\n",
    "**Decoder**. The decoder is a neural language model which conditions on the encoder output $h_i$. The computation is similar to that of the encoder except we introduce matrices $C_z$, $C_r$ and $C$ that are used to bias the update gate, reset gate and hidden state computation by the sentence vector. One decoder is used for the next sentence $s_{i+1}$ while a second decoder is used for the previous sentence $s_{i-1}$. Separate parameters are used for each decoder with the exception of the vocabulary matrix $V$, which is the weight matrix connecting the decoder’s hidden state for computing a distribution over words. In what follows we describe the decoder for the next sentence $s_{i+1}$ although an analogous\n",
    "computation is used for the previous sentence $s_{i-1}$. Let $h_{i+1}^t$ denote the hidden state of the decoder at time $t$. Decoding involves iterating through the following sequence of equations (dropping the subscript $i + 1$):\n",
    "\n",
    ">> (5) $r^t = \\sigma(W_r^d x^{t-1} + U_r^d h^{t-1} + C_r h_i)$\n",
    "\n",
    ">> (6) $z^t = \\sigma(W_z^d x^{t-1} + U_z^d h^{t-1} + C_ z h_i)$\n",
    "\n",
    ">> (7) $\\overline{h^t} = tanh(W^ d x^{t-1} + U^d (r^t \\odot h^{t-1}) + C h_i)$\n",
    "\n",
    ">> (8) $h_{i +1}^t = (1 - z^t) \\odot h^{t-1} + z^t \\odot \\overline{h^t}$\n",
    "\n",
    "> Given $h_{i+1}^t$, the probability of word $w_{i+1}^t$ given the previous $t-1$ words and the encoder vector is\n",
    "\n",
    ">> (9) $P(w_{i+1}^t \\mid w_{i+1}^{<t}; h_i) \\propto exp(v_{w_{i+1}^t} h_{i+1}^t)$\n",
    "\n",
    "> where $v_{w_{i+1}^t}$ denotes the row of $V$ corresponding to the word of $w_{i+1}^t$. An analogous computation is performed for the previous sentence $s_{i-1}$.\n",
    "\n",
    "**Objective**. Given a tuple ($s_{i-1}; s_i; s_{i+1}$), the objective optimized is the sum of the log-probabilities for the forward and backward sentences conditioned on the encoder representation:\n",
    "\n",
    ">> (10) $\\sum_{t} log P(w_{i+1}^t \\mid w_{i+1}^{<t}; h_i) + \\sum_{t} log P(w_{i-1}^t \\mid w_{i-1}^{<t}; h_i)$\n",
    "\n",
    "The total objective is the above summed over all such training tuples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
